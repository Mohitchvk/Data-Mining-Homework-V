{"cells":[{"cell_type":"markdown","metadata":{"id":"Yq0XoVUIVNLC"},"source":["# Homework 5"]},{"cell_type":"markdown","metadata":{"id":"0aqF485OxR-U"},"source":["**Before you start:** Read Chapter on Naive Bayes and KNN in the textbook.\n","\n","**Note:** Please enter the code along with your comments in the **TODO** section.\n","\n","Alternative solutions are always welcomed."]},{"cell_type":"markdown","metadata":{"id":"Rion40Hwnf-B"},"source":["# Part 1: K-Nearst-Neighbors"]},{"cell_type":"markdown","metadata":{"id":"6X-yZb6YvfhH"},"source":["### Problem 2 ##"]},{"cell_type":"markdown","metadata":{"id":"1hgf-yTV9q2V"},"source":["The objective is to classify the breast cancer data using K-NN classifier."]},{"cell_type":"markdown","metadata":{"id":"7zY4oMVmBMHg"},"source":["**TODO1**\n","\n","Load the breast cancer data and rename the columns to the below fields in the same order\n","\n","Id, C_thickness, Cell_Size, Cell_Shape, Adhesion, E_Cell_Size, Bare_Nuclei, B_Chromatin, N_Nucleoli, Mitoses, Class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kVAjSQpZYiJ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"oSlUegOdcT2I"},"source":["**TODO 2**\n","\n","Plot the heatmap for the correlation coefficients with the target variable (Class)  and interpret your findings.\n","\n","\n","Drop redundant columns and view summary of the dataset.\n","Convert all the variables to numeric.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6GyYUSKkfjXG"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"2saGofKTEpiJ"},"source":["**TODO 3**\n","\n","\n","\n","Considering the fundamental idea of k-NN, would you recommend data rescaling before model building? Why? \n","\n","If so, partition the data into 75% training and 25% validation set.\n","\n","Impute the missing values with the mean values of training data.\n","Check if all the nulls are removed in both train and test dataset.\n","\n","Standardize the data.\n","\n","**Note:**   When you standardize the validation set, you need to use the training set's mean and variance. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mizcZxd1fips"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"RrGgm_3mJbIR"},"source":["**TODO 4**\n","\n","Choose the best k from 1-10 based on the classification accuracy of different k values on the validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lgT0rqC4fOAp"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"p7nvr1AbXeMg"},"source":["**TODO 5**\n","\n","For the chosen k, display the confusion matrix and evaluate the performance of the model using recall and precision.\n","\n","Check for overfitting and underfitting for the k chosen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UANl3XmTfPjZ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"rvUBy4oPMMIf"},"source":["**TODO 6**\n","\n","Classify the new record given below using the chosen k. \n","\n","1002945, 5, 4, 4, 5, 7, 10, 3, 2, 1\n","\n","Considering the size of the dataset, would you recommend data partition before scoring the new record? Why?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NtThVQwSfhow"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"XvDQjUmNSuLZ"},"source":["### Problem 3 ##"]},{"cell_type":"markdown","metadata":{"id":"96_tZKmHSuLZ"},"source":["The data concerns city-cycle fuel consumption in miles per gallon (mpg). The objective is to use k-NN regression to predict the mpg with the given attributes."]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":88,"metadata":{"id":"viZfphuao7EV"},"outputs":[],"source":["# import the dataset \"auto_mpg.csv\"\n","df=pd.read_csv(\"auto_mpg.csv\")"]},{"cell_type":"markdown","metadata":{"id":"H2OO76SGo7EV"},"source":["**TODO 1**\n","\n","Check the unique value of the variable \"car name\". \n","\n","Would you recommend keeping \"car name\" for prediction? Why? \n","\n","If not, eliminate the variable \"car name\"."]},{"cell_type":"code","execution_count":89,"metadata":{"id":"05R9WLrTpijX"},"outputs":[{"data":{"text/plain":["Index(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n","       'acceleration', 'model year', 'origin', 'car name'],\n","      dtype='object')"]},"execution_count":89,"metadata":{},"output_type":"execute_result"}],"source":["df.columns"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique Count of the variable car name:  301\n","Length of the Dataset:  393\n"]}],"source":["print(\"Unique Count of the variable car name: \",len(df['car name'].unique()))\n","print(\"Length of the Dataset: \",len(df))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Here, we can see that almost all the values are unique except a few. So, keeping this column does not serve any purpose to the model. So its better to drop the predictor variable help in reducing the complexity of the model.  \n","\n","Using One Hot encoding will impart 301 additional columns and  label encoding this nominal variable will bias the model by adding weight to the value."]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":["df.drop('car name', axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"H8XjK4Fro7EW"},"source":["**TODO 2**\n","\n","Convert the variable \"origin\" to dummy variables before modeling"]},{"cell_type":"code","execution_count":92,"metadata":{"id":"FnkWQelTpkZI"},"outputs":[],"source":["df = pd.get_dummies(df, columns=[\"origin\"], drop_first=True)"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mpg</th>\n","      <th>cylinders</th>\n","      <th>displacement</th>\n","      <th>horsepower</th>\n","      <th>weight</th>\n","      <th>acceleration</th>\n","      <th>model year</th>\n","      <th>origin_2</th>\n","      <th>origin_3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>18.0</td>\n","      <td>8</td>\n","      <td>307.0</td>\n","      <td>130</td>\n","      <td>3504</td>\n","      <td>12.0</td>\n","      <td>70</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>15.0</td>\n","      <td>8</td>\n","      <td>350.0</td>\n","      <td>165</td>\n","      <td>3693</td>\n","      <td>11.5</td>\n","      <td>70</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>18.0</td>\n","      <td>8</td>\n","      <td>318.0</td>\n","      <td>150</td>\n","      <td>3436</td>\n","      <td>11.0</td>\n","      <td>70</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>16.0</td>\n","      <td>8</td>\n","      <td>304.0</td>\n","      <td>150</td>\n","      <td>3433</td>\n","      <td>12.0</td>\n","      <td>70</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17.0</td>\n","      <td>8</td>\n","      <td>302.0</td>\n","      <td>140</td>\n","      <td>3449</td>\n","      <td>10.5</td>\n","      <td>70</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>388</th>\n","      <td>27.0</td>\n","      <td>4</td>\n","      <td>140.0</td>\n","      <td>86</td>\n","      <td>2790</td>\n","      <td>15.6</td>\n","      <td>82</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>389</th>\n","      <td>44.0</td>\n","      <td>4</td>\n","      <td>97.0</td>\n","      <td>52</td>\n","      <td>2130</td>\n","      <td>24.6</td>\n","      <td>82</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>390</th>\n","      <td>32.0</td>\n","      <td>4</td>\n","      <td>135.0</td>\n","      <td>84</td>\n","      <td>2295</td>\n","      <td>11.6</td>\n","      <td>82</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>391</th>\n","      <td>28.0</td>\n","      <td>4</td>\n","      <td>120.0</td>\n","      <td>79</td>\n","      <td>2625</td>\n","      <td>18.6</td>\n","      <td>82</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>392</th>\n","      <td>31.0</td>\n","      <td>4</td>\n","      <td>119.0</td>\n","      <td>82</td>\n","      <td>2720</td>\n","      <td>19.4</td>\n","      <td>82</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>393 rows Ã— 9 columns</p>\n","</div>"],"text/plain":["      mpg  cylinders  displacement horsepower  weight  acceleration  \\\n","0    18.0          8         307.0        130    3504          12.0   \n","1    15.0          8         350.0        165    3693          11.5   \n","2    18.0          8         318.0        150    3436          11.0   \n","3    16.0          8         304.0        150    3433          12.0   \n","4    17.0          8         302.0        140    3449          10.5   \n","..    ...        ...           ...        ...     ...           ...   \n","388  27.0          4         140.0         86    2790          15.6   \n","389  44.0          4          97.0         52    2130          24.6   \n","390  32.0          4         135.0         84    2295          11.6   \n","391  28.0          4         120.0         79    2625          18.6   \n","392  31.0          4         119.0         82    2720          19.4   \n","\n","     model year  origin_2  origin_3  \n","0            70         0         0  \n","1            70         0         0  \n","2            70         0         0  \n","3            70         0         0  \n","4            70         0         0  \n","..          ...       ...       ...  \n","388          82         0         0  \n","389          82         1         0  \n","390          82         0         0  \n","391          82         0         0  \n","392          82         0         0  \n","\n","[393 rows x 9 columns]"]},"execution_count":93,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"NNx7iZ2aC8h1"},"source":["**TODO 3**\n","\n","Partition the data into 75% training and 25% validation set."]},{"cell_type":"code","execution_count":119,"metadata":{"id":"CGAWagCrDGDA"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","x_train, x_valid, y_train, y_valid = train_test_split(\n","    df.drop(\"mpg\", axis=1), df[\"mpg\"], test_size=0.25)\n"]},{"cell_type":"markdown","metadata":{"id":"gcjzpN38o7EY"},"source":["**TODO 4**\n","\n","Rescale the numeric data. Note that dummy variables should not be rescaled.\n","\n","**Note:** When you standardize the validation set, you need to use the training set's mean and variance."]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[],"source":["x_train['horsepower'] = x_train['horsepower'].replace('?', 0)\n","x_valid['horsepower'] = x_valid['horsepower'].replace('?', 0)\n"]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","\n","X_train_scaled = scaler.fit_transform(x_train.iloc[:,:-2])"]},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","\n","X_train_scaled = scaler.fit_transform(x_train.iloc[:,:-2])\n","x_train_last_two_cols = x_train.iloc[:, -2:].reset_index(drop=True)\n","X_train_scaled = pd.concat([pd.DataFrame(X_train_scaled, columns=x_train.iloc[:,:-2].columns), x_train_last_two_cols], axis=1)\n","\n","X_valid_scaled = scaler.transform(x_valid.iloc[:,:-2])\n","x_valid_last_two_cols = x_valid.iloc[:, -2:].reset_index(drop=True)\n","X_valid_scaled = pd.concat([pd.DataFrame(X_valid_scaled,columns=x_train.iloc[:,:-2].columns),x_valid_last_two_cols], axis=1)"]},{"cell_type":"markdown","metadata":{"id":"ezvy0kvYo7EZ"},"source":["**TODO 5**\n","\n","Choose the best k from 1-10 based on the MSE of different k values on the validation set. Explain the reason for your choice."]},{"cell_type":"code","execution_count":133,"metadata":{"id":"Agdd1CYPp2rC"},"outputs":[{"name":"stdout","output_type":"stream","text":["MSE for different k values:\n","k = 1 : MSE = 17.366060606060607\n","k = 2 : MSE = 13.316565656565656\n","k = 3 : MSE = 12.06654320987654\n","k = 4 : MSE = 11.762569444444445\n","k = 5 : MSE = 11.436351515151514\n","k = 6 : MSE = 11.214941077441075\n","k = 7 : MSE = 11.837701504844361\n","k = 8 : MSE = 11.617659406565656\n","k = 9 : MSE = 12.340900361641099\n","k = 10 : MSE = 12.441959595959595\n","Best k value: 6\n","MSE for best k value: 11.214941077441075\n"]}],"source":["from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","\n","k_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","\n","mse_dict = {}\n","\n","for k in k_values:\n","    knn = KNeighborsRegressor(n_neighbors=k)\n","    knn.fit(X_train_scaled, y_train)\n","    y_pred = knn.predict(X_valid_scaled)\n","    mse = mean_squared_error(y_valid, y_pred)\n","    mse_dict[k] = mse\n","\n","print(\"MSE for different k values:\")\n","for k, mse in mse_dict.items():\n","    print(\"k =\", k, \": MSE =\", mse)\n","\n","best_k = min(mse_dict, key=mse_dict.get)\n","\n","print(\"Best k value:\", best_k)\n","print(\"MSE for best k value:\", mse_dict[best_k])"]},{"cell_type":"code","execution_count":134,"metadata":{},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAvzklEQVR4nO3deXxU9bn48c+ThSwkJJAFgQCBBCJ7UFAJVcEF3Kpete5Vq/1p67W3ti4VbatVa1Ws0uXWVlsvXq1erFpbd1xAFHAB2VfZJGHLAiGB7Mnz++NMMISEbDNzZnKe9+v1fc3MOSfn+8yIz5z5nu95jqgqxhhjvCPC7QCMMcYElyV+Y4zxGEv8xhjjMZb4jTHGYyzxG2OMx1jiN8YYj7HEb1wnIn1FZIGIlIvIbwOw/0wRURGJ8r1+W0SubbL+QREpFpHdvtf/ISL5InJARMb7O56uaP5eAtjPbBF5MJB9GPdY4jeHEZFtIlIjIqnNli/3JZxM3+sMEXnFlzD3i8gqEbnOt64xOR1o1i5rpdsbgWKgl6reFsC3B4Cqnq2qz/piHQjcBoxU1WN8mzwG3KKqCaq6LNDxNCUi80Xk+8Hs03hPQI8aTNjaClwB/AFARMYAcc22eQ5YAQwGqoExwDHNtklW1bp29DcYWKuduJpQRKLa2cfR+i5R1cJmy9Z0Zmd+iMeYgLMjftOS54Brmry+FvjfZttMBGar6kFVrVPVZar6dkc7EpHZvv3f6ftVcIaIxIjILBHZ6WuzRCTGt/0UESkQkZ/5hmb+p4V9RorIY75fI1uAc5utny8i3xeRM4D3gP6+vl8UkQNAJLBCRDb7tu/v+3VTJCJbReS/muzrPhF5WUSeF5Ey4DoRSRKRv4nILhHZ4RtKivRtf52IfOKLb59vf2f71v0aOBn4oy+eP7bj87vY9yttdAvr1onIeU1eR/k+k+N8r/8hIrt9v9gWiMioVvq4TkQ+abZMRSTb9zzG9362i8geEfmziMT51qWKyBsiUioie0XkYxGxvOMy+w9gWvIp0EtERvgS1mXA8y1s898icrmIDOpsR6p6HfB34FHf0Mr7wD3ASUAuMA44Afh5kz87BuiDc2R+Ywu7/X/AecB4YAJwSSt9vw+cDez09X2Fqib4Vo9T1Sxfknod59fNAOB04FYRmd5kVxcALwPJvvfyLFAHZPtimAY0Hb45EdgApAKPAn8TEVHVe4CP+WaY6ZZWPjYAROR7wCPAGaq6uoVNXsT55dZoOlCsql/6Xr8NDAPSgS99sXfGI8BwnP9e2Tif0y99624DCoA0oC9wN2B1Ylxmid+0pvGo/0xgPbCj2frv4CSpXwBbfecAJjbbpth3pNfYRrSz76uA+1W1UFWLgF8B322yvgG4V1WrVbWyhb+/FJilqvmquhf4TTv7bclEIE1V71fVGlXdAjwNXN5km8Wq+pqqNgC9cL5MbvX9GioEnmi2/deq+rSq1uN8SfTDSYodcStwBzBFVTe1ss0LwPkiEu97faVvGQCq+oyqlqtqNXAfME5EkjoShIgIzhftT1R1r6qWAw/xzfutxXl/g1W1VlU/7syQnvEvG+M3rXkOWAAM4chhHlR1H3AXcJfvRPBjwGsiktFks9ROjnf3B75u8vpr37JGRapa1cbf5zf7+84ajDMUVNpkWSTOl16j/GbbRwO7nJwIOAdYTbfZ3fhEVSt82yXQMXfgfDkWtLaBqm4SkXXAt0XkdeB8nF8g+H7J/RrnCzwN58sUnF8h+zsQRxoQDyxt8n4F5zMCmInzpTLXt/4pVX24A/s3AWCJ37RIVb8Wka3AOcANbWxbLCKP4YzV9/FD9zs5/ATrIN+yQ1228fe7gIFNXnd6KAonYW9V1WFH2aZpPPk4J7s7+6XX3qPhacA7IrJbVV85ynaNwz0ROCfQG38dXIkzRHUGsA1IAvbhJO3mDuIkdwBEpOlJ/GKgEhilqs1/FeL7BXAbcJvvHMI8EflCVT9o17s0AWFDPeZobgBOU9WDzVeIyCMiMtp3wjAR+CGwSVVL/NDvi8DPRSTN92vilxx5juFoXgL+S5wpp71xfpl01udAme9kcpzvxPHoFoa1AFDVXcBc4Lci0ktEIkQkS0RObWd/e4Ch7dhuDXAWznmW84+y3f/hfEn8kCbDPEAizhdUCU5Sf+go+1gBjBKRXBGJxTmCB8A3vPU08ISIpAOIyIDGcyAicp6IZPuGhMqAel8zLrLEb1qlqptVdUkrq+OBfwKlwBacI/TmCahUDp/H/9N2dv0gsARYCazCOfHYkYuJngbexUlYXwKvduBvD+Mbh/82zonLrThHuH/FOUJuzTVAD2AtzlH0yzjj3O3xO+AS34yf37cR2wqck9hPN84MamGbXcBiIA+Y02TV/+IMge3wxfnpUfrZCNwPvA98BXzSbJOfAZuAT30zm94HcnzrhvleH/DF8SdVnX+092UCT+w8izHGeIsd8RtjjMdY4jfGGI+xxG+MMR5jid8YYzwmLObxp6amamZmptthGON/GzY4jzk5R9/OmE5YunRpsaqmNV8eFok/MzOTJUtam1VoTBibMsV5nD/fzShMNyUiLV61bkM9xhjjMWFxxG9Mt/Xzn7e9jTF+ZonfGDedcYbbERgPssRvjJuWL3cec3PdjCIoamtrKSgooKrqaIVVTWfExsaSkZFBdHR0u7a3xG+Mm2691Xn0wMndgoICEhMTyczMpEkJZ9NFqkpJSQkFBQUMGTKkXX/TbRP/a8t2MPPdDewsraR/chx3TM/hwvED3A7LGM+qqqqypB8AIkJKSgpFRUXt/ptumfhfW7aDGa+uorLWqf66o7SSGa+uArDkb4yLLOkHRkc/1245nXPmuxsOJf1GlbX1zHx3g0sRGWNM6OiWiX9naUu3YW19uTHGGyIjI8nNzWXcuHEcd9xxLFq0qFP7mTVrFhUVFS2uy8zMpLi4uCthHmH+/Pnk5uYyatQoTj21vff0aV23HOrpnxzHjhaSfP/kOBeiMeYoHjraja+Mv8XFxbHcN5Pq3XffZcaMGXz00Ucd3s+sWbO4+uqriY+Pb3vjLiotLeXmm2/mnXfeYdCgQRQWFnZ5n93yiP+O6TnERUcetiwuOpI7pls9FBNi8vKcZoKurKyM3r17H3o9c+ZMJk6cyNixY7n33nsBOHjwIOeeey7jxo1j9OjRzJkzh9///vfs3LmTqVOnMnXq1Fb3X1lZyVlnncXTTz/dpThfeOEFLrroIgYNcm4dnZ6e3qX9QTc94m88gTvz3Q3sKK0kOlL4zUVj7MSuCT2NQw1eTP6NdYqauvRSuPlmqKiAc845cv111zmtuBguueTwde2YEltZWUlubi5VVVXs2rWLDz/8EIC5c+fy1Vdf8fnnn6OqnH/++SxYsICioiL69+/Pm2++CcD+/ftJSkri8ccfZ968eaSmprbYz4EDB7j88su55ppruOaaa45Yf9lll7Fhw5HnHH/6058esf3GjRupra1lypQplJeX8+Mf/7jFfXZEt0z84CT/C8cP4PH3NvLHD79i6rFd/5Y0xu/uvtt59MA8/lDQdKhn8eLFXHPNNaxevZq5c+cyd+5cxo8fDziJ+6uvvuLkk0/m9ttv52c/+xnnnXceJ598crv6ueCCC7jzzju56qqrWlw/Z86cFpe3pK6ujqVLl/LBBx9QWVnJpEmTOOmkkxg+fHi799Fct038jSZnpfD7D77isy0lTBt1jNvhGGMaHe3LLj7+6OtTU7v8ZTlp0iSKi4spKipCVZkxYwY33XTTEdstXbqUt956ixkzZjBt2jR++ctftrnvyZMn8/bbb3PllVe2ONWyI0f8GRkZpKam0rNnT3r27Mkpp5zCihUrupT4u+UYf1O5g5KJjY5g0eYSt0MxxoSQ9evXU19fT0pKCtOnT+eZZ57hwIEDAOzYsYPCwkJ27txJfHw8V199NbfffjtffvklAImJiZSXl7e67/vvv5+UlBRuvvnmFtfPmTOH5cuXH9FaGsK54IIL+Pjjj6mrq6OiooLPPvuMESNGdOm9d/sj/pioSCZm9mHRZv9OrzLGhJ/GMX5wSh08++yzREZGMm3aNNatW8ekSZMASEhI4Pnnn2fTpk3ccccdREREEB0dzZNPPgnAjTfeyNlnn02/fv2YN29ei33NmjWL66+/njvvvJNHH3200zGPGDGCs846i7FjxxIREcH3v/99Ro8e3en9AYiqdmkHwTBhwgTtyo1Ynpy/mUfeWc/n95xOemKsHyMzpos8dCOWdevWdflI1bSupc9XRJaq6oTm2wbsiF9EngHOAwpVdbRv2RygcU5lMlCqqrmBiqHR5OwUABZvLuGCXJvZY0LIrFluR2A8KJBj/LOBs5ouUNXLVDXXl+xfAV4NYP+HjOqfRK/YKBZtsnF+E2Jycz1RktmEloAd8avqAhHJbGmdOKe5LwVOC1T/TUVGCCcNTWHRFhvnNyHm/fedR4/ckEVVrVBbAHR0yN6tWT0nA3tU9avWNhCRG0VkiYgs6Ui50dbkZaWQv7eS/L0t19cwxhUPPug0D4iNjaWkpKTDScocXWM9/tjY9p+/dGtWzxXAi0fbQFWfAp4C5+RuVzucnO1cYbdoczGX9RnU1d0ZYzooIyODgoKCDtWNN+3TeAeu9gp64heRKOAi4Phg9pudnkBaYgwLN5Vw2URL/MYEW3R0dLvvEGUCy42hnjOA9apaEMxORYS8rBQWbbafmsYYbwtY4heRF4HFQI6IFIjIDb5Vl9PGME+g5GWlUHygmq8KD7jRvTHGhIRAzuq5opXl1wWqz7bkZTnj/As3FTO8b6JbYRjzjb/8xe0IjAd1+1o9TQ3sE8+gPvFWt8eEjpwcpxkTRJ5K/OAM93y6pYS6+ga3QzEGXn/dacYEkfcSf3Yq5VV1rNlZ5nYoxsBvf+s0Y4LIc4l/0lCnbs9Cq9ZpjPEozyX+tMQYcvomstjG+Y0xHuW5xA8wKSuFL7btpbqu3u1QjDEm6DyZ+Cdnp1JV28Cy7aVuh2KMMUHX7e/A1ZIThvQhQmDRpmJO8o35G+OK555zOwLjQZ484k+Ki2ZMRrLN5zfuGzjQacYEkScTPzjz+Zfnl3Kwus7tUIyXzZnjNGOCyLOJf3JWKnUNyufb9rodivGyJ590mjFB5NnEf/zg3vSIjGDRJpvPb4zxFs8m/rgekRw32Mb5jTHe49nED061zrW7yth3sMbtUIwxJmg8nfgnZ6egCp9usaN+Y4x3eHIef6OxGcn07BHJws3FnD2mn9vhGC96+WW3IzAe5OnEHx0ZwQlD+tg4v3FPaqrbERgP8vRQDzjlG7YUHWT3/iq3QzFeNHu204wJIs8n/klZTsmGRVam2bjBEr9xgecT/4hjetE7PpqFm2y4xxjjDZ5P/BERwqSsFBZtLkZV3Q7HGGMCzvOJH5z5/Lv2V7GtpMLtUIwxJuAs8eMUbANYaOUbjDEeELDELyLPiEihiKxutvxHIrJBRNaIyKOB6r8jhqT2pF9SrN2O0QTfW285zZggCuQR/2zgrKYLRGQqcAEwVlVHAY8FsP92E/lmnL+hwcb5TRDFxzvNmCAKWOJX1QVA85rHPwQeVtVq3zaFgeq/oyZnpbKvopb1u8vdDsV4yZ/+5DRjgijYY/zDgZNF5DMR+UhEJra2oYjcKCJLRGRJUVFRwAPLy7b5/MYFL73kNGOCKNiJPwroDZwE3AG8JCLS0oaq+pSqTlDVCWlpaQEPrF9SHENTe1r5BmNMtxfsxF8AvKqOz4EGIGSKlUzKSuGzLSXU1je4HYoxxgRMsBP/a8BpACIyHOgBhMzYyuTsVA7W1LOyYL/boRhjTMAEcjrni8BiIEdECkTkBuAZYKhviuf/AddqCF0ue9JQ3zi/zec3xnRjASvLrKpXtLLq6kD12VV9evZgZL9eLNpcwo9OH+Z2OMYL5s93OwLjQXblbjN5WSks3b6Pqtp6t0MxxpiAsMTfzOTsVGrqGlj69T63QzFe8NhjTjMmiCzxNzNxSB+iIsTq9pjgeOMNpxkTRJb4m0mIiWLcwGSbz2+M6bYs8bdgclYKKwtKKauqdTsUY4zxO0v8LZiUlUqDwudbmpcaMsaY8GeJvwXHDU4mJiqChVa3xwRaXJzTjAmigM3jD2cxUZFMzOxj9flN4L39ttsRGA+yI/5W5GWnsH53OcUHqt0OxRhj/MoSfyvyspzacXbUbwLqgQecZkwQWeJvxej+vUiMjbL6/CawPvjAacYEkSX+VkRFRnDikBQWbrIjfmNM92KJ/ygmZ6ewfW8F+Xsr3A7FGGP8xhL/Udg4vzGmO7LEfxTD+yaQmtDDxvlN4KSkOM2YILJ5/EchIkzKSmXh5hJUlVZuD2xM573yitsRGA+yI/42TM5Koai8ms1FB9wOxRhj/MISfxsax/ltdo8JiBkznGZMEFnib8OglHgyesfZOL8JjMWLnWZMEFnib4fJWaks3lxCfUPI3BfeGGM6zRJ/O+Rlp1BWVcfanWVuh2KMMV1mib8dJmU50+2sTLMxpjuwxN8O6YmxDEtPsNsxGv/LyHCaMUEUsMQvIs+ISKGIrG6y7D4R2SEiy33tnED172+Ts1P5Yuteauoa3A7FdCfPP+80Y4IokEf8s4GzWlj+hKrm+tpbAezfryZlpVBZW8/y/FK3QzHGmC4JWOJX1QVAt7lp7UlDU4gQWLjJxvmNH916q9OMCSI3xvhvEZGVvqGg3q1tJCI3isgSEVlSVFQUzPhalBQXzegBSVawzfjX8uVOMyaIgp34nwSygFxgF/Db1jZU1adUdYKqTkhLSwtSeEeXl5XKsvx9VNTUuR2KMcZ0WlATv6ruUdV6VW0AngZOCGb/XZWXlUJtvfLFtn1uh2KMMZ0W1MQvIv2avPwPYHVr24aiiZl9iI4UFtk4vzEmjAWsLLOIvAhMAVJFpAC4F5giIrmAAtuAmwLVfyDE9Yhk/KDeNp/f+M/w4W5HYDwoYIlfVa9oYfHfAtVfsEzOSmXWBxspraghOb6H2+GYcPfUU25HYDzIrtztoLzsFFTh0y3dZqaqMcZjLPF30LiMZOJ7RFqZZuMfN97oNGOCyG692EE9oiKYmNnHxvmNf2zc6HYExoPsiL8TJmensKnwAHvKqtwOxRhjOswSfyc03o7RhnuMMeHIEn8njOzXi+T4aBbZfXiNMWHIxvg7ISJCmDQ0hUWbS1BVRMTtkEy4ys11OwLjQZb4OykvK4W3V+9m+94KBqf0dDscE65mzXI7AuNBNtTTSXnZzjj/QhvuMcaEmaMmfhG5usnzyc3W3RKooMLB0NSe9O0VYyd4TddcfbXTjAmito74f9rk+R+arbvez7GEFRFhclYqizeX0NCgbodjwlVBgdOMCaK2Er+08ryl154zKSuFkoM1bCwsdzsUY4xpt7YSv7byvKXXnmPj/MaYcNRW4j/Wd5vEVU2eN77OCUJ8IW1AchyZKfEstnF+Y0wYaWs654igRBHG8rJTeX35TurqG4iKtElSpoMmTXI7AuNBR038qvp109cikgKcAmxX1aWBDCxc5GWl8MJn21m1Yz/jB7V673hjWvab37gdgfGgtqZzviEio33P++HcKvF64DkRuTXw4YW+SUNTAKxapzEmbLQ1NjFEVRvvi/s94D1V/TZwIh6fztkoJSGGY49JtPn8pnMuvthpxgRRW4m/tsnz04G3AFS1HGgIVFDhZnJ2Kku27aOqtt7tUEy4KSlxmjFB1FbizxeRH4nIfwDHAe8AiEgcEB3o4MJFXlYK1XUNfLl9n9uhGGNMm9pK/DcAo4DrgMtUtdS3/CTgfwIXVng5YUgfIiPEyjQbY8JCW7N6CoEftLB8HjAvUEGFm8TYaMZlJPnG+T1/eYMxJsQdNfGLyL+Ptl5Vz/dvOOErLyuVJz/aTHlVLYmxNgpm2un0092OwHhQWxdwTQLygReBz+hAfR4ReQY4DyhU1dHN1t0OzATSVLVbTIfJy07hj/M28cW2vZx2bF+3wzHh4he/cDsC40FtjfEfA9wNjAZ+B5wJFKvqR6r6URt/Oxs4q/lCERno28/2Dkcbwo4b1JuYqAir22OMCXlHTfyqWq+q76jqtTgndDcB80XkR23tWFUXAHtbWPUEcCfdrMhbbHQkEzJ724VcpmPOPttpxgRRm8VlRCRGRC4Cngf+E/g98GpnOhOR84EdqrqiHdveKCJLRGRJUVFRZ7oLurysVNbtKqPkQLXboZhwUVnpNGOCqK2SDc8Ci3Dm8P9KVSeq6gOquqOjHYlIPHAP8Mv2bK+qT6nqBFWdkJaW1tHuXJGX5ZRvWLzFjvqNMaGrrSP+7wLDgR8Di0SkzNfKRaSsg31lAUOAFSKyDcgAvhSRYzoadKgaMyCJxJgoG+4xxoS0tubx+63OsKquAtIbX/uS/4TuMqsHICoyghOH9mHRpm7zlowx3VDACsiLyIvAYiBHRApE5IZA9RVKJmWlsq2kgh2lNm5r2uG885xmTBC1NY+/01T1ijbWZwaqbzdNzvaVad5UzHcmDHQ5GhPybr/d7QiMB9kto/xseHoiKT17sNjG+Y0xIcoSv59FRAiTslJYuLkY1W51qYIJhClTnGZMEFniD4C8rFT2lFWzpfig26EYY8wRLPEHQNNxfmOMCTWW+ANgUJ94BiTH2Xx+Y0xIssQfACJCXlYKi7eU0NBg4/zGmNBiiT9AJmenUlpRy9pdHb3A2XjKpZc6zZggCtg8fq+b5Kvbs2hzMaMHJLkcjQlZN9/sdgTGg+yIP0D69oolOz3BxvnN0VVUOM2YILLEH0B5WSl8vnUvNXUNbodiQtU55zjNmCCyxB9AeVmpVNTUs7Kg1O1QjDHmEEv8AXTS0D6IYLdjNMaEFEv8AZQc34PR/ZNYtNku5DLGhA5L/AGWltCDz7buZchdbzL54Q95bVmHb15mjDF+ZdM5A+i1ZTv4xDfMo8CO0kpmvLoKgAvHD3AxMhMyrrvO7QiMB1niD6CZ726gpv7wGT2VtfXMfHeDJX7jsMRvXGBDPQG0s5W7cLW23HhQcbHTjAkiS/wB1D85rkPLjQddconTjAkiS/wBdMf0HOKiIw9bFiHw0zOHuRSRMcZY4g+oC8cP4DcXjWFAchwCJMdH06Cw5OtSuzuXMcY1dnI3wC4cP+CwE7mPvLOeJ+dvJjs9gRu+NcTFyIwxXmWJP8jumJbDlqID/PrNtQxJjee0Y/u6HZIxxmNsqCfIIiKEJy7LZWT/XvzohWWs3231+j3thz90mjFBFLDELyLPiEihiKxusuwBEVkpIstFZK6I9A9U/6EsvkcUf71mIgmxUdwwewlF5dVuh2TcctllTjMmiAJ5xD8bOKvZspmqOlZVc4E3gF8GsP+QdkxSLH+9ZiIlB6u56bklVNXWux2ScUN+vtOMCaKAJX5VXQDsbbas6bhGT5xKBp41JiOJJy7N5cvtpfzslZU208eLvvtdpxkTREEf4xeRX4tIPnAVHj7ib3T2mH7cMT2Hfy3fyR8+3OR2OMYYDwh64lfVe1R1IPB34JbWthORG0VkiYgsKSoqCl6ALrh5ShYXjR/A4+9t5I2VO90OxxjTzbk5q+cF4OLWVqrqU6o6QVUnpKWlBTGs4BMRfnPxGCYM7s1tL61geX6p2yEZY7qxoCZ+EWlaq+B8YH0w+w9lMVGR/OW7x5PeK4b/979LrJCbMSZgAjmd80VgMZAjIgUicgPwsIisFpGVwDTgx4HqPxylJMTwt2snUlVTzw3PLuFgdZ3bIZlAu+02pxkTRBIOM0kmTJigS5YscTuMoJm/oZDrZ3/Bacf25S/fPZ7ICHE7JGNMGBKRpao6oflyu3I3BE3JSeeX543k/XV7ePQdGw3r1jZscJoxQWS1ekLUtXmZbC46yF8WbCErLYFLJw50OyQTCDfd5DzOn+9qGMZb7Ig/RIkI9357JCcPS+Xuf67i0y0lbodkjOkmLPGHsKjICP545XEMTonnB88vZVvxQbdDMsZ0A5b4Q1xSXDTPXDcRAa5/9gv2V9S6HZIxJsxZ4g8Dg1N68uerjyd/bwX/+cKX1NY3uB2SMSaMWeIPEycOTeGh/xjDJ5uKue/fa6ygW3fx8587zZggslk9YeQ7Ewayueggf/7IuXXj9ybbrRvD3hlnuB2B8SBL/GHmzunOrRsfeGMtmak9mZqT7nZIpiuWL3cec3PdjMJ4jA31hJmICGHW5bmM6OfcunHD7nK3QzJdceutTjOmmdeW7WDywx8y5K43mfzwh7y2bIff9m2JPwzF94jir9dOIL5HJNfP/oLiA3brRmO6k9eW7WDGq6vYUVqJAjtKK5nx6iq/JX9L/GGqX1Icf712gu/WjUvt1o3GdBOqyiPvrKey2f/TlbX1zHzXP+U9bIw/jI3NSObxS3O5+e9fctcrK3nislxErKCbMaGopq6BogPVFJVXU1hWRdGBagrLqiksd5YVlVc5jweqqa1vedaev8q1W+IPc+eM6cft04bz2NyNZKcncMtpw9r+I2NMq15btoOZ725gZ2kl/ZPjuGN6DheOH9DitqpKWVWdk8wbE7evFTZZVlheTWkrF1+m9OxBWmIMaYkxZKcnkt4rhhc++5r9lUeWZe+fHOeX92iJvxv4z6nZbC46yGNzNzIkNYFzx/ZzOyTTXg895HYEponGsfXGYZYdpZXc+fJKFm8uJqN3/KGj88Lyb47Yq+uOvKCyR1QEaQkxpPeKYUhqT04Y0of0xFjSEmNIT4w59DwloQfRkUeOuOf0TTwsDoC46EjumJ7jl/dpib8bEBEevngM2/dWcNs/lpPRO45xA5PdDsu0R16e2xGYJma+e+TYek19A3OWFABOCZV039H58YN6+xJ5LOm9Yg4l+rSEWHrFRXVp2LXxF0Z7f3l0lN2IpRspPlDNhf+9kJq6Bv51y2T6JfnnZ6EJoEWLnEf7AnDdvoM1jH/gvRbXCbDugbOIjY4MblBdZDdi8YDUhBieuW4iFTX13DDbbt0YFu6+22nGVfPWFzJt1oJW1/dPjgu7pH80lvi7meF9E/nDleNZv7uMn8xZTkND6P+iM8YtB6vrmPHqKr43+wv6xPfg9unDiWuW4P05th4qLPF3Q1Nz0vnFeSOZu3YPj/pp3q8x3c0X2/Zy9u8+5v++2M5Npw7l3z+azC1Th/Gbi8YwIDkOAQYkx/Gbi8b4bWw9VNjJ3W7qurxMNhcd4M8fbWZoWk8unWC3bjQGoLqunsfnbuSpj7eQ0TuOl26axMTMPofWXzh+QLdL9M1Z4u+mnFs3jmJbcQX3/HMVg/rEc9LQFLfDMsZVa3bu56dzVrBhTzlXnDCIe84dQUKM99Kgzerp5vZX1nLRnxays7SSXnHRFJZV+31qmOkCq84ZFHX1DfxlwRZmvb+R5PgePHrxWKYe2/0r27Y2q8d7X3UekxQXzeUTB/Hrt9ZRWesUc2ss+ARY8nebJfyA21p8kJ++tJxl20s5d0w/HrxwNL179nA7LFcF7OSuiDwjIoUisrrJspkisl5EVorIP0UkOVD9m2/MXrTtiGWVtfU89NY6u5OX295/32nG71SV5xZv45zffczmwgP87vJc/njleM8nfQjsrJ7ZwFnNlr0HjFbVscBGYEYA+zc+rRV2Kiyv5luPzOOef67i/bV7qKixef9B9+CDTjN+tXt/Fdc88zm/+NcaJmT2Zu5PTuWC3AFWxNAnYEM9qrpARDKbLZvb5OWnwCWB6t98o39yHDtaSP7JcdGM6t+Lfy7bwd8/206PyAhOHNqHqTnpTD02nSGpPV2I1pjOU1X+vWInv3htNbX1ygMXjubqEwdZwm/GzTH+64E5ra0UkRuBGwEGDRoUrJi6pTum57RY8Om+80dx4fgBVNfVs2TbPuatL2TehkLuf2Mt97+xlsyUeKb4vgROHNKnW125aLqffQdr+Plrq3lz1S6OG+SULM+0g5cWBXRWj++I/w1VHd1s+T3ABOAibUcANqun6zpSanZ7SQXzNxYyb30hizaXUF3XQGx0BJOzUplybDpThqcxsE98kN9BNzVlivM4f76bUYS9D9fv4WevrKK0ooafnDmcm07JIjLCjvJDZlaPiFwLnAec3p6kb/yjIxelDEqJ55pJmVwzKZOq2noWbylh/vpC5m0o4oP1hQAMS09gqu9LYEJmH3pE2UXg4aojBwWh5kB1Hb9+cy0vfp7Pscck8uz3TmBk/15uhxXygnrELyJnAY8Dp6pqUXv3Y0f8oUFV2VJ8kPkbipi/oZDPtuylpr6Bnj0i+dawVKbmpDMlJ51jkmLdDjV8bPCV1MhxpxZM8/rz4AwDhkOZgs+37uW2fyynYF8lN52SxU/OHEZMlA1HNtXaEX/AEr+IvAhMAVKBPcC9OLN4YoAS32afquoP2tqXJf7QdLC6jkWbS5i3oZD56wvZub8KgBH9ejE1J42px6YzfmAyUU1uNBHOR5fd0eSHP2zxxH96YgyfzjidiBAcLqmqreeJ95ySCwN7x/PbS8cdVnLBfCPoid+fLPGHPlVl454DzNvgnBtY8vU+6huUXrFRnDI8jak56VTU1vHQm+vD8ugyYF5/3Xn89reD3nVtfQPD7nm71fWJsVGMy0hmbEYS4wYmMy4j2fVfc01LLlx54iDuOWcEPT1YcqG9LPGboNpfWcvCTcXMW1/I/I1FFJVXt7rtgOQ4Ft51WhCjCyEundz95Kti7nt9DZsKD7S4PjkumnPG9mNlQSnrd5VT5yvv3bdXDGMzkskd6HwhjB2QTFJ8dMDjbVpyoXd8Dx65ZCxTc7p/yYWuCpmTu8YbkuKiOWdMP84Z04+GBmXtrjLO+8MnLW67o7SSmroGO0EcBAX7Kvj1m+t4e/VuBqfE8/2Th/D3T7e3OtUXnKGVtbvKWJFfysqC/azIL+W9tXsObT8ktSfjMpIYm5HMuIHJjOrfy69Tf5uWXDhvbD8euMBKLnSVJX4TcBERwugBSQxo5UIygOMffI+pOemcObIvU3LSSIwN/FGkl1TV1vPUgi38af4mAG6fNpzvnzyU2OhIRvdPOup5l9joSI4b1JvjBvU+tGx/ZS2rCvazoqCUFfmlfLplL68t3wlAVISQc0yib3jIGSYalp7Y4emVqspzn37NQ2+tIyYqkt9fMZ7zx/X3w6dhbKjHBE1LM0hioyO46sTBlFfV8v66QvYerCE6UpiUlcqZI/ty5oi+ro8rB1SAh3pUlffXFXL/G2vI31vJuWP6cfe5IxiQ7P/7Me8pq2JFfqnvy2A/KwtKKatyyoDERUcyZkDSofMFuQOTyegdd9gVtU1P/PftFUNSXDQb9hzglOFpPHrx2O797yBAbIzfhISjzeqpb1C+3L6P99buYe6a3WwrqQBgXEYSZ47sy7RRxzAsPaF7XX4fwMS/pegAv3p9LR9tLGJYegK/On8Uedmpfu+nNQ0NyraSg6ws2M/y/FJWFpSyemcZNXUNAPSOj2bcwGTGZiRTWVPHc59+TVVtw2H7uOT4DGZeMrZ7/TcPIkv8JqyoKpsKDzB37R7mrt3DivxSAAanxDNtZF/OHHkMxw/uHf5XZ+bnO48D/XeHtIPVdfzhw0387ZMtxEZFcuuZw7lm0mCiI90/h1Jb38CG3eWsKChlZb4zVLRxTzmt3Rra0yf+/cASvwlre8qqeH/dHuau2cPizSXU1DfQp2cPTj/WOS9w8rA04np4++IdVeX1lbt46M117C6r4pLjM7jzrBzSE0N7iKSipo5Rv3yXljKRAFsfPjfYIXUbNqvHhLW+vWK56sTBh84HLNhYzNy1u3lnzW7+sbSA2OgITh6Wxpkj+3L6semkJMS4HXL7zPHVKbzssi7tZt2uMu779xo+27qX0QN68d9XHcfxg3u3/YchIL5HVKsVZPsH4FyEsSN+E+Zq6xv4fOte5q7ZzXtr97BzfxURAhMG93FODo/sG9oVGrs4xr+/opYn3t/I/y7eRlJcNHdMP5bLJg4MuyGwcC4dEcpsqMd0e6rKmp1lzF27h/fW7mHdrjIAhvdNcE4OjzyGMQOS+PeKnaFTNqKTib+hQfnH0nweeWcDpRU1XHXiYG6bNpzk+PCd327lPPzPEr/xnPy9Fbzn+xL4fNteXwmJSA7WNFDf5Gyiq0eWnUj8y/NLufdfq1lRsJ8Jg3vzqwtGMap/UkDCM+HNxviN5wzsE8/13xrC9d8aQmlFDR+uL+Tuf646LOmDc//h+19fwwlD+tAvKTZkpw4WH6hm5jsbmLMkn/TEGGZdlssFuf1DNl4TuizxG09Iju/BRcdlcNtLK1pcv7eilryHPyQtMYZxGcnkDvSVIMgITi2ao6mrb+C5T7/m8fc2UllTz02nDOVHpw8jwYqTmU6yfznGU1qbPZKWEMMtp2UfuvL0/XVH1qIZN9CpRTOynx9r0bz88lFXf7qlhPv+vYb1u8s5eVgq9357FNnpCf7p23iWJX7jKa3df/iec0ccNsa/v7KW1TucK05bqkVzbL9ExvmKkuUOTCYrLaFzM2lSW76Sdtf+Sh56az2vr9jJgOQ4/nz18Uwf1deGdYxf2Mld4zmdnT2ye3/VoaJkjVeellc7tWh69ohkTEbSoS+DcQOT6d+e8wWzZzuP110HQHVdPX/7ZCt//HAT9Q3KD07N4genZnn+4jTTOTarxxg/a2hwbkW50vdlsLxgP+t2llFT79SbSU2IIXfgN18GYzOSDptu+dqyHQy6+Fxq6uq57ebfce6YY3hvXSFbiw8ybWRffnHeSLupvekSm9VjjJ9FRAjZ6Qlkpydw0XEZgHPEvn5X+aEKlc75gsJDf5OZEs+4gclEAG+t3s2zdc6Q047SSp76eCtpCT149voTOHV4mhtvyXiEJX5j/CgmKvLQUA+TnGVlVbWsLtjPct8vg8+27GV3WVWLfx8dGWFJ3wScJX5jAqxXbDR52amHlUQectebLRYl27W/5S8EY/zJ/TqtxnhQa8XHrCiZCQY74jfGBY3TSq/7zn2HlsVFR3LH9Bz3gjKeYYnfGBc0Th9tnFY6wIqSmSAK2FCPiDwjIoUisrrJsu+IyBoRaRCRI6YYGeMlF44fwMJe69k66GsW3nWaJX0TNIEc458NnNVs2WrgImBBAPs1Jny89JLTjAmigA31qOoCEclstmwdYJedG2OMi0J2Vo+I3CgiS0RkSVFRkdvhGGNMtxGyiV9Vn1LVCao6IS3NLmgxxhh/CdnEb4wxJjDCYjrn0qVLi0Xka7fj6KJUoNjtIEKIfR7fSEXEPotv2L+Nw3Xl8xjc0sKAVecUkReBKThB7wHuBfYCfwDSgFJguapOD0gAIUZElrRUJc+r7PP4hn0Wh7PP43CB+DwCOavnilZW/TNQfRpjjGmbjfEbY4zHWOIPnqfcDiDE2OfxDfssDmefx+H8/nmExR24jDHG+I8d8RtjjMdY4jfGGI+xxB9gIjJQROaJyDpfZdIfux2T20QkUkSWicgbbsfiNhFJFpGXRWS979/IJLdjcouI/MT3/8hqEXlRRGLdjimYWqlo3EdE3hORr3yPvf3RlyX+wKsDblPVEcBJwH+KyEiXY3Lbj4F1bgcRIn4HvKOqxwLj8OjnIiIDgP8CJqjqaCASuNzdqIJuNkdWNL4L+EBVhwEf+F53mSX+AFPVXar6pe95Oc7/2J4tvC4iGcC5wF/djsVtItILOAX4G4Cq1qhqqatBuSsKiBORKCAe2OlyPEGlqgtwLnJt6gLgWd/zZ4EL/dGXJf4g8pWpHg985nIobpoF3Ak0uBxHKBgKFAH/4xv6+quI9HQ7KDeo6g7gMWA7sAvYr6pz3Y0qJPRV1V3gHEQC6f7YqSX+IBGRBOAV4FZVLXM7HjeIyHlAoaoudTuWEBEFHAc8qarjgYP46ad8uPGNXV8ADAH6Az1F5Gp3o+q+LPEHgYhE4yT9v6vqq27H46LJwPkisg34P+A0EXne3ZBcVQAUqGrjL8CXcb4IvOgMYKuqFqlqLfAqkOdyTKFgj4j0A/A9Fvpjp5b4A0yc2439DVinqo+7HY+bVHWGqmaoaibOibsPVdWzR3WquhvIF5Ec36LTgbUuhuSm7cBJIhLv+3/mdDx6oruZfwPX+p5fC/zLHzsNi7LMYW4y8F1glYgs9y27W1Xfci8kE0J+BPxdRHoAW4DvuRyPK1T1MxF5GfgSZybcMjxWuqFpRWMRKcCpaPww8JKI3IDz5fgdv/RlJRuMMcZbbKjHGGM8xhK/McZ4jCV+Y4zxGEv8xhjjMZb4jTHGYyzxG9MJIpLZtIqiMeHEEr8xxniMJX5jukhEhvqKrE10OxZj2sMSvzFd4Cu38ArwPVX9wu14jGkPK9lgTOel4dROuVhV17gdjDHtZUf8xnTefiAfpx6TMWHDjviN6bwanDsivSsiB1T1BZfjMaZdLPEb0wWqetB3g5n3ROSgqvqlbK4xgWTVOY0xxmNsjN8YYzzGEr8xxniMJX5jjPEYS/zGGOMxlviNMcZjLPEbY4zHWOI3xhiP+f/78LPZB+bUOQAAAABJRU5ErkJggg==","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","\n","# Plot the MSE values for each k value\n","plt.plot(list(mse_dict.keys()), list(mse_dict.values()),marker='o',)\n","\n","# Add axis labels and a title to the plot\n","plt.xlabel('k')\n","plt.ylabel('MSE')\n","plt.title('MSE for different k values')\n","\n","# Add a vertical line to indicate the best k value\n","plt.axvline(x=best_k, linestyle='--', color='red', label='Best k = {}'.format(best_k))\n","\n","# Add a legend to the plot\n","plt.legend()\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"MLdN-jMIp4-2"},"source":["**TODO 6**\n","\n","\n","Score the validation set with the best k. Comment on the model performance."]},{"cell_type":"code","execution_count":138,"metadata":{"id":"dmCunSm7p85I"},"outputs":[{"name":"stdout","output_type":"stream","text":["R2 score on the validation set: 1.0\n","MSE score on the validation set: 11.214941077441075\n","RMSE score on the validation set: 3.3488716125646074\n"]}],"source":["# Train a KNN model using the best k value\n","best_knn = KNeighborsRegressor(n_neighbors=best_k)\n","best_knn.fit(X_train_scaled, y_train)\n","\n","# Use the trained model to make predictions on the validation set\n","y_pred = best_knn.predict(X_valid_scaled)\n","\n","# Compute the MSE of the predictions on the validation set\n","r2 = best_knn.score(X_valid_scaled, y_pred)\n","mse = mean_squared_error(y_valid, y_pred)\n","rmse = mean_squared_error(y_valid, y_pred, squared=False)\n","# Print the MSE score of the predictions on the validation set\n","\n","print(\"R2 score on the validation set:\", r2)\n","print(\"MSE score on the validation set:\", mse)\n","print(\"RMSE score on the validation set:\", rmse)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The R2 score here shows that the model performs very well on the validation set with a k value of 6 "]},{"cell_type":"markdown","metadata":{"id":"T5fTwsd1UKSN"},"source":["# Naive Bayes"]},{"cell_type":"markdown","metadata":{"id":"3IKYRqKtxR-N"},"source":["### **Problem 4**##"]},{"cell_type":"markdown","metadata":{"id":"H8fENJRPk7P7"},"source":["In this problem, we need to build a Naive Bayes model to classify whether a movie review is positive or negative. \n","\n","The given data is a subset of [the IMDB movie review dataset](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).\n","\n","This might be your first time working with text mining. Therefore, the basic pre-processing steps are given below. \n","\n","**You have two major tasks:**\n","\n","* Go through the code and get to know the purpose of each preprocessing step. Summarize what a preprocessing step does when required.\n","* Build a multinomial Naive Bayes model to classify the reviews."]},{"cell_type":"code","execution_count":139,"metadata":{"id":"gsuUyuEhrcQO"},"outputs":[],"source":["# # Please remove # and run the following code if you have an error while importing the dataset\n","# !pip install --upgrade openpyxl"]},{"cell_type":"code","execution_count":140,"metadata":{"id":"C8gmUJ3n8Mir"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. <br /><br />The...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive"]},"execution_count":140,"metadata":{},"output_type":"execute_result"}],"source":["# Import the dataset\n","import pandas as pd\n","# from google.colab import files\n","# file = files.upload()\n","df = pd.read_csv(\"IMDB Dataset_subset.csv\")\n","df.head()"]},{"cell_type":"code","execution_count":141,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":402,"status":"ok","timestamp":1616434292565,"user":{"displayName":"Wei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBWSIyrP45L1xQ6JBkXSATlg4BOmkV28NZZ96I=s64","userId":"06017554584660737110"},"user_tz":240},"id":"ggt8c71U8MrD","outputId":"7f5ae431-5205-4812-9d73-e0060bd6c653"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\mohit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\mohit\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":141,"metadata":{},"output_type":"execute_result"}],"source":["# Packages required for preprocessing #\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.stem import WordNetLemmatizer #for lemmatization\n","import re #regular expression package\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":142,"metadata":{"id":"QMI-UZAQ9F-L"},"outputs":[],"source":["X = [row for row in df['review']] #list of reviews\n","classes = df['sentiment'] #list of true classes"]},{"cell_type":"code","execution_count":143,"metadata":{"id":"E4P3iseB_gIR"},"outputs":[],"source":["# Pre-process the data\n","reviews = []\n","lemmatizer = WordNetLemmatizer() \n","\n","for review in range(0, len(X)):\n","    # part 1\n","    review = re.sub(r'[\\W_]', ' ', str(X[review])) \n","    review = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', review) \n","    review = re.sub(r'\\^[a-zA-Z]\\s+', ' ', review) \n","    review = re.sub(r'\\s+', ' ', review, flags=re.I) \n","    review = re.sub(r'^b\\s+', '', review) # if a review record is in bytes, the corresponding line will have a letter 'b' appended at the start)\n","    review = review.lower()\n","    review = re.sub(r'[0-9]+', '', review) \n","\n","    # part 2\n","    review = review.split()\n","    review = [lemmatizer.lemmatize(word) for word in review]\n","    review = ' '.join(review)\n","\n","    reviews.append(review)"]},{"cell_type":"markdown","metadata":{"id":"orhex-OXhkgV"},"source":["\n","**TODO 1**\n","\n","Explain the function that part 1 and part 2 achieve in the loop."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rGnaXwAfBv8i"},"source":["This code is performing data preprocessing for a text data in the variable X using various techniques including:\n","\n","* Removing non-alphabetic characters, punctuation, and numbers.\n","* Converting all characters to lowercase.\n","* Splitting the text into individual words.\n","* Lemmatizing each word to its base form.\n","* Joining the lemmatized words back into a single string.\n","* The processed reviews are then stored in the reviews list. This preprocessing step is typically performed before training a machine learning model for text  * analysis, such as sentiment analysis or text classification.\n"]},{"cell_type":"code","execution_count":144,"metadata":{"id":"dXexNUV9CnXZ"},"outputs":[],"source":["# Continue with pre-processing\n","vectorizer = CountVectorizer(stop_words = \"english\", max_df=0.7, min_df=5) \n","texts = vectorizer.fit_transform(reviews).toarray()  \n","vocab = vectorizer.vocabulary_ \n","vocab = sorted(vocab.items(), key = lambda x: x[1])\n","vocab = [v[0] for v in vocab]"]},{"cell_type":"markdown","metadata":{"id":"nZPj-AWCC-YV"},"source":["\n","**TODO 2**\n","\n","What do \"texts\" and \"vocab\" represent? What is the relationship between them?"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gOho20diDt6L"},"source":["texts is a matrix of word frequencies, where each row represents a document (in this case, a preprocessed review) and each column represents a word in the vocabulary. The value in each cell represents the frequency of the corresponding word in the corresponding document. This is obtained by using the CountVectorizer method to convert the preprocessed reviews stored in reviews into a bag-of-words representation, where each word is represented by a unique index.\n","\n","vocab is a list of the words in the vocabulary, sorted by their corresponding index in texts. In other words, each element of vocab corresponds to a column in texts, and the order of the elements in vocab matches the order of the columns in texts.\n","\n","Thus, texts and vocab are related by the fact that texts is a matrix representation of the reviews using the words in vocab, where each row corresponds to a review and each column corresponds to a word. The CountVectorizer method has essentially transformed the text data into a numerical representation that can be used as input to machine learning models."]},{"cell_type":"markdown","metadata":{"id":"wkjhBNIlGFlu"},"source":["**TODO 3**\n","\n","Partition the data into 80% training and 20% validation set."]},{"cell_type":"code","execution_count":155,"metadata":{},"outputs":[],"source":["y=pd.get_dummies(df.iloc[:,-1:], drop_first=True)"]},{"cell_type":"code","execution_count":161,"metadata":{"id":"5WeDE--MGMq5"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(pd.DataFrame(texts), y, test_size=0.2)\n"]},{"cell_type":"markdown","metadata":{"id":"ZpGQxB2GGUzz"},"source":["**TODO 4**\n","\n","Build a multinomial Naive Bayes model on the training set."]},{"cell_type":"code","execution_count":162,"metadata":{"id":"mJGgsv-JGvga"},"outputs":[{"name":"stderr","output_type":"stream","text":["e:\\anaconda3\\envs\\mohit_chodisetti\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"data":{"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"],"text/plain":["MultinomialNB()"]},"execution_count":162,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.naive_bayes import MultinomialNB\n","\n","nb_classifier = MultinomialNB()\n","nb_classifier.fit(X_train, y_train)\n"]},{"cell_type":"markdown","metadata":{"id":"zyypSMm_GzGb"},"source":["**Hint:** [Multinomial Naive Bayes with sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"]},{"cell_type":"markdown","metadata":{"id":"vfIyk3vMGww0"},"source":["**TODO 5**\n","\n","Evaluate the model performance with the training and validation set. Comment on the model performance."]},{"cell_type":"code","execution_count":164,"metadata":{"id":"B5TyEr9pHJGt"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training accuracy: 0.9246875\n","Training precision: 0.9250227987652146\n","Training recall: 0.9246875\n","Training F1-score: 0.9246553252718043\n"]}],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","train_preds = nb_classifier.predict(X_train)\n","\n","train_acc = accuracy_score(y_train, train_preds)\n","train_prec = precision_score(y_train, train_preds, average='weighted')\n","train_rec = recall_score(y_train, train_preds, average='weighted')\n","train_f1 = f1_score(y_train, train_preds, average='weighted')\n","\n","print(\"Training accuracy:\", train_acc)\n","print(\"Training precision:\", train_prec)\n","print(\"Training recall:\", train_rec)\n","print(\"Training F1-score:\", train_f1)\n"]},{"cell_type":"code","execution_count":166,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation accuracy: 0.83625\n","Validation precision: 0.8370941176470589\n","Validation recall: 0.83625\n","Validation F1-score: 0.836093976616147\n"]}],"source":["val_preds = nb_classifier.predict(X_valid)\n","\n","val_acc = accuracy_score(y_valid, val_preds)\n","val_prec = precision_score(y_valid, val_preds, average='weighted')\n","val_rec = recall_score(y_valid, val_preds, average='weighted')\n","val_f1 = f1_score(y_valid, val_preds, average='weighted')\n","\n","print(\"Validation accuracy:\", val_acc)\n","print(\"Validation precision:\", val_prec)\n","print(\"Validation recall:\", val_rec)\n","print(\"Validation F1-score:\", val_f1)"]},{"cell_type":"markdown","metadata":{"id":"aQy7IX2xHK1-"},"source":["**Hint:** [Classification report with sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The Naive Bayes model performs well on both the training and validation set, with high accuracy and F1-score values. However, there is a slight drop in performance from the training set to the validation set, which indicates that the model may be slightly overfitting on the training data. Overall, an accuracy of 83.6% on the validation set is a decent performance, but it can be improved by adjusting the model hyperparameters or trying different classification algorithms.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uIRgUPM8HiOK"},"source":["**If you are interested (this part is not graded):**\n","\n","Explore one or two records that were misclassified. Check the original text, vectorized text, and comment on the possible reason why the record got misclassified."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"REELGHe8IFcZ"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["6X-yZb6YvfhH","XvDQjUmNSuLZ","3IKYRqKtxR-N"],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}
