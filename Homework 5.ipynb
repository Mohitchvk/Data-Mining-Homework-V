{"cells":[{"cell_type":"markdown","metadata":{"id":"Yq0XoVUIVNLC"},"source":["# Homework 5"]},{"cell_type":"markdown","metadata":{"id":"0aqF485OxR-U"},"source":["**Before you start:** Read Chapter on Naive Bayes and KNN in the textbook.\n","\n","**Note:** Please enter the code along with your comments in the **TODO** section.\n","\n","Alternative solutions are always welcomed."]},{"cell_type":"markdown","metadata":{"id":"Rion40Hwnf-B"},"source":["# Part 1: K-Nearst-Neighbors"]},{"cell_type":"markdown","metadata":{"id":"6X-yZb6YvfhH"},"source":["### Problem 2 ##"]},{"cell_type":"markdown","metadata":{"id":"1hgf-yTV9q2V"},"source":["The objective is to classify the breast cancer data using K-NN classifier."]},{"cell_type":"markdown","metadata":{"id":"7zY4oMVmBMHg"},"source":["**TODO1**\n","\n","Load the breast cancer data and rename the columns to the below fields in the same order\n","\n","Id, C_thickness, Cell_Size, Cell_Shape, Adhesion, E_Cell_Size, Bare_Nuclei, B_Chromatin, N_Nucleoli, Mitoses, Class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kVAjSQpZYiJ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"oSlUegOdcT2I"},"source":["**TODO 2**\n","\n","Plot the heatmap for the correlation coefficients with the target variable (Class)  and interpret your findings.\n","\n","\n","Drop redundant columns and view summary of the dataset.\n","Convert all the variables to numeric.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6GyYUSKkfjXG"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"2saGofKTEpiJ"},"source":["**TODO 3**\n","\n","\n","\n","Considering the fundamental idea of k-NN, would you recommend data rescaling before model building? Why? \n","\n","If so, partition the data into 75% training and 25% validation set.\n","\n","Impute the missing values with the mean values of training data.\n","Check if all the nulls are removed in both train and test dataset.\n","\n","Standardize the data.\n","\n","**Note:**   When you standardize the validation set, you need to use the training set's mean and variance. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mizcZxd1fips"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"RrGgm_3mJbIR"},"source":["**TODO 4**\n","\n","Choose the best k from 1-10 based on the classification accuracy of different k values on the validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lgT0rqC4fOAp"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"p7nvr1AbXeMg"},"source":["**TODO 5**\n","\n","For the chosen k, display the confusion matrix and evaluate the performance of the model using recall and precision.\n","\n","Check for overfitting and underfitting for the k chosen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UANl3XmTfPjZ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"rvUBy4oPMMIf"},"source":["**TODO 6**\n","\n","Classify the new record given below using the chosen k. \n","\n","1002945, 5, 4, 4, 5, 7, 10, 3, 2, 1\n","\n","Considering the size of the dataset, would you recommend data partition before scoring the new record? Why?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NtThVQwSfhow"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"XvDQjUmNSuLZ"},"source":["### Problem 3 ##"]},{"cell_type":"markdown","metadata":{"id":"96_tZKmHSuLZ"},"source":["The data concerns city-cycle fuel consumption in miles per gallon (mpg). The objective is to use k-NN regression to predict the mpg with the given attributes."]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"viZfphuao7EV"},"outputs":[],"source":["# import the dataset \"auto_mpg.csv\"\n","df=pd.read_csv(\"auto_mpg.csv\")"]},{"cell_type":"markdown","metadata":{"id":"H2OO76SGo7EV"},"source":["**TODO 1**\n","\n","Check the unique value of the variable \"car name\". \n","\n","Would you recommend keeping \"car name\" for prediction? Why? \n","\n","If not, eliminate the variable \"car name\"."]},{"cell_type":"code","execution_count":51,"metadata":{"id":"05R9WLrTpijX"},"outputs":[{"data":{"text/plain":["Index(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n","       'acceleration', 'model year', 'origin', 'car name'],\n","      dtype='object')"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["df.columns"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"data":{"text/plain":["amc matador            5\n","ford pinto             5\n","ford maverick          5\n","toyota corolla         5\n","toyota corona          4\n","                      ..\n","buick skyhawk          1\n","chevrolet monza 2+2    1\n","ford mustang ii        1\n","pontiac astro          1\n","chevy s-10             1\n","Name: car name, Length: 301, dtype: int64"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["df['car name'].value_counts()"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["df.drop('car name', axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"H8XjK4Fro7EW"},"source":["**TODO 2**\n","\n","Convert the variable \"origin\" to dummy variables before modeling"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"FnkWQelTpkZI"},"outputs":[],"source":["df = pd.get_dummies(df, columns=[\"origin\"], drop_first=True)"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mpg</th>\n","      <th>cylinders</th>\n","      <th>displacement</th>\n","      <th>horsepower</th>\n","      <th>weight</th>\n","      <th>acceleration</th>\n","      <th>model year</th>\n","      <th>origin_2</th>\n","      <th>origin_3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>18.0</td>\n","      <td>8</td>\n","      <td>307.0</td>\n","      <td>130</td>\n","      <td>3504</td>\n","      <td>12.0</td>\n","      <td>70</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>15.0</td>\n","      <td>8</td>\n","      <td>350.0</td>\n","      <td>165</td>\n","      <td>3693</td>\n","      <td>11.5</td>\n","      <td>70</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>18.0</td>\n","      <td>8</td>\n","      <td>318.0</td>\n","      <td>150</td>\n","      <td>3436</td>\n","      <td>11.0</td>\n","      <td>70</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>16.0</td>\n","      <td>8</td>\n","      <td>304.0</td>\n","      <td>150</td>\n","      <td>3433</td>\n","      <td>12.0</td>\n","      <td>70</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17.0</td>\n","      <td>8</td>\n","      <td>302.0</td>\n","      <td>140</td>\n","      <td>3449</td>\n","      <td>10.5</td>\n","      <td>70</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>388</th>\n","      <td>27.0</td>\n","      <td>4</td>\n","      <td>140.0</td>\n","      <td>86</td>\n","      <td>2790</td>\n","      <td>15.6</td>\n","      <td>82</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>389</th>\n","      <td>44.0</td>\n","      <td>4</td>\n","      <td>97.0</td>\n","      <td>52</td>\n","      <td>2130</td>\n","      <td>24.6</td>\n","      <td>82</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>390</th>\n","      <td>32.0</td>\n","      <td>4</td>\n","      <td>135.0</td>\n","      <td>84</td>\n","      <td>2295</td>\n","      <td>11.6</td>\n","      <td>82</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>391</th>\n","      <td>28.0</td>\n","      <td>4</td>\n","      <td>120.0</td>\n","      <td>79</td>\n","      <td>2625</td>\n","      <td>18.6</td>\n","      <td>82</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>392</th>\n","      <td>31.0</td>\n","      <td>4</td>\n","      <td>119.0</td>\n","      <td>82</td>\n","      <td>2720</td>\n","      <td>19.4</td>\n","      <td>82</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>393 rows Ã— 9 columns</p>\n","</div>"],"text/plain":["      mpg  cylinders  displacement horsepower  weight  acceleration  \\\n","0    18.0          8         307.0        130    3504          12.0   \n","1    15.0          8         350.0        165    3693          11.5   \n","2    18.0          8         318.0        150    3436          11.0   \n","3    16.0          8         304.0        150    3433          12.0   \n","4    17.0          8         302.0        140    3449          10.5   \n","..    ...        ...           ...        ...     ...           ...   \n","388  27.0          4         140.0         86    2790          15.6   \n","389  44.0          4          97.0         52    2130          24.6   \n","390  32.0          4         135.0         84    2295          11.6   \n","391  28.0          4         120.0         79    2625          18.6   \n","392  31.0          4         119.0         82    2720          19.4   \n","\n","     model year  origin_2  origin_3  \n","0            70         0         0  \n","1            70         0         0  \n","2            70         0         0  \n","3            70         0         0  \n","4            70         0         0  \n","..          ...       ...       ...  \n","388          82         0         0  \n","389          82         1         0  \n","390          82         0         0  \n","391          82         0         0  \n","392          82         0         0  \n","\n","[393 rows x 9 columns]"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"NNx7iZ2aC8h1"},"source":["**TODO 3**\n","\n","Partition the data into 75% training and 25% validation set."]},{"cell_type":"code","execution_count":56,"metadata":{"id":"CGAWagCrDGDA"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(\n","    df.drop(\"mpg\", axis=1), df[\"mpg\"], test_size=0.25)\n"]},{"cell_type":"markdown","metadata":{"id":"gcjzpN38o7EY"},"source":["**TODO 4**\n","\n","Rescale the numeric data. Note that dummy variables should not be rescaled.\n","\n","**Note:** When you standardize the validation set, you need to use the training set's mean and variance."]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["x_train['horsepower'] = x_train['horsepower'].replace('?', np.nan)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","\n","X_train_scaled = scaler.fit_transform(x_train.iloc[:,:-2])\n","x_train_last_two_cols = x_train.iloc[:, -2:]\n","X_train_scaled = pd.concat([pd.DataFrame(X_train_scaled), x_train_last_two_cols], axis=1)\n","\n","X_valid_scaled = scaler.transform(x_test.iloc[:,:-2])\n","x_valid_last_two_cols = x_test.iloc[:, -2:]\n","X_valid_scaled = pd.concat([pd.DataFrame(X_valid_scaled),x_valid_last_two_cols], axis=1)"]},{"cell_type":"markdown","metadata":{"id":"ezvy0kvYo7EZ"},"source":["**TODO 5**\n","\n","Choose the best k from 1-10 based on the MSE of different k values on the validation set. Explain the reason for your choice."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Agdd1CYPp2rC"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"MLdN-jMIp4-2"},"source":["**TODO 6**\n","\n","\n","Score the validation set with the best k. Comment on the model performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dmCunSm7p85I"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"T5fTwsd1UKSN"},"source":["# Naive Bayes"]},{"cell_type":"markdown","metadata":{"id":"3IKYRqKtxR-N"},"source":["### **Problem 4**##"]},{"cell_type":"markdown","metadata":{"id":"H8fENJRPk7P7"},"source":["In this problem, we need to build a Naive Bayes model to classify whether a movie review is positive or negative. \n","\n","The given data is a subset of [the IMDB movie review dataset](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).\n","\n","This might be your first time working with text mining. Therefore, the basic pre-processing steps are given below. \n","\n","**You have two major tasks:**\n","\n","* Go through the code and get to know the purpose of each preprocessing step. Summarize what a preprocessing step does when required.\n","* Build a multinomial Naive Bayes model to classify the reviews."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gsuUyuEhrcQO"},"outputs":[],"source":["# # Please remove # and run the following code if you have an error while importing the dataset\n","# !pip install --upgrade openpyxl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C8gmUJ3n8Mir"},"outputs":[],"source":["# Import the dataset\n","import pandas as pd\n","from google.colab import files\n","file = files.upload()\n","df = pd.read_csv(\"IMDB Dataset_subset.csv\")\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":402,"status":"ok","timestamp":1616434292565,"user":{"displayName":"Wei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBWSIyrP45L1xQ6JBkXSATlg4BOmkV28NZZ96I=s64","userId":"06017554584660737110"},"user_tz":240},"id":"ggt8c71U8MrD","outputId":"7f5ae431-5205-4812-9d73-e0060bd6c653"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# Packages required for preprocessing #\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.stem import WordNetLemmatizer #for lemmatization\n","import re #regular expression package\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QMI-UZAQ9F-L"},"outputs":[],"source":["X = [row for row in df['review']] #list of reviews\n","classes = df['sentiment'] #list of true classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4P3iseB_gIR"},"outputs":[],"source":["# Pre-process the data\n","reviews = []\n","lemmatizer = WordNetLemmatizer() \n","\n","for review in range(0, len(X)):\n","    # part 1\n","    review = re.sub(r'[\\W_]', ' ', str(X[review])) \n","    review = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', review) \n","    review = re.sub(r'\\^[a-zA-Z]\\s+', ' ', review) \n","    review = re.sub(r'\\s+', ' ', review, flags=re.I) \n","    review = re.sub(r'^b\\s+', '', review) # if a review record is in bytes, the corresponding line will have a letter 'b' appended at the start)\n","    review = review.lower()\n","    review = re.sub(r'[0-9]+', '', review) \n","\n","    # part 2\n","    review = review.split()\n","    review = [lemmatizer.lemmatize(word) for word in review]\n","    review = ' '.join(review)\n","\n","    reviews.append(review)"]},{"cell_type":"markdown","metadata":{"id":"orhex-OXhkgV"},"source":["\n","**TODO 1**\n","\n","Explain the function that part 1 and part 2 achieve in the loop."]},{"cell_type":"markdown","metadata":{"id":"rGnaXwAfBv8i"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXexNUV9CnXZ"},"outputs":[],"source":["# Continue with pre-processing\n","vectorizer = CountVectorizer(stop_words = \"english\", max_df=0.7, min_df=5) \n","texts = vectorizer.fit_transform(reviews).toarray()  \n","vocab = vectorizer.vocabulary_ \n","vocab = sorted(vocab.items(), key = lambda x: x[1])\n","vocab = [v[0] for v in vocab]"]},{"cell_type":"markdown","metadata":{"id":"nZPj-AWCC-YV"},"source":["\n","**TODO 2**\n","\n","What do \"texts\" and \"vocab\" represent? What is the relationship between them?"]},{"cell_type":"markdown","metadata":{"id":"gOho20diDt6L"},"source":[]},{"cell_type":"markdown","metadata":{"id":"wkjhBNIlGFlu"},"source":["**TODO 3**\n","\n","Partition the data into 80% training and 20% validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5WeDE--MGMq5"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ZpGQxB2GGUzz"},"source":["**TODO 4**\n","\n","Build a multinomial Naive Bayes model on the training set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJGgsv-JGvga"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"zyypSMm_GzGb"},"source":["**Hint:** [Multinomial Naive Bayes with sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"]},{"cell_type":"markdown","metadata":{"id":"vfIyk3vMGww0"},"source":["**TODO 5**\n","\n","Evaluate the model performance with the training and validation set. Comment on the model performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5TyEr9pHJGt"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"aQy7IX2xHK1-"},"source":["**Hint:** [Classification report with sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"]},{"cell_type":"markdown","metadata":{"id":"uIRgUPM8HiOK"},"source":["**If you are interested (this part is not graded):**\n","\n","Explore one or two records that were misclassified. Check the original text, vectorized text, and comment on the possible reason why the record got misclassified."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"REELGHe8IFcZ"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["6X-yZb6YvfhH","XvDQjUmNSuLZ","3IKYRqKtxR-N"],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}
